
环境的状态S, t时刻环境的状态St是它的环境状态集中某一个状态。
个体的动作A, t时刻个体采取的动作At是它的动作集中某一个动作。
环境的奖励R, t时刻个体在状态St采取的动作At对应的奖励Rt+1会在t+1时刻得到。
个体的策略(policy)π,它代表个体采取动作的依据，即个体会依据策略π来选择动作。
    最常见的策略表达方式是一个条件概率分布π(a|s), 即在状态s时采取动作a的概率。即π(a|s)=P(At=a|St=s).此时概率大的动作被个体选择的概率较高。
个体在策略π和状态s时，采取行动后的价值（value），一般用vπ(s)表示。
    这个价值一般是一个期望函数。虽然当前动作会给一个延时奖励Rt+1,但是光看这个延时奖励是不行的，因为当前的延时奖励高，不代表到了t+1,t+2,...时刻的后续奖励也高。
    比如下象棋，我们可以某个动作可以吃掉对方的车，这个延时奖励是很高，但是接着后面我们输棋了。此时吃车的动作奖励值高但是价值并不高。
    因此我们的价值要综合考虑当前的延时奖励和后续的延时奖励。
    价值函数vπ(s)一般可以表示为下式，不同的算法会有对应的一些价值函数变种，但思路相同。：
    vπ(s)=Eπ(Rt+1+γRt+2+γ2Rt+3+...|St=s)
奖励衰减因子γ，在[0，1]之间。如果为0，则是贪婪法，即价值只由当前延时奖励决定，如果是1，则所有的后续状态奖励和当前奖励一视同仁。
    大多数时候，我们会取一个0到1之间的数字，即当前延时奖励的权重比后续奖励的权重大。
环境的状态转化模型，可以理解为一个概率状态机，它可以表示为一个概率模型，即在状态s下采取动作a,转到下一个状态s′的概率，表示为Pass′。
探索率ϵ，这个比率主要用在强化学习训练迭代过程中，由于我们一般会选择使当前轮迭代价值最大的动作，但是这会导致一些较好的但我们没有执行过的动作被错过。
    因此我们在训练选择最优动作时，会有一定的概率ϵ不选择使当前轮迭代价值最大的动作，而选择其他的动作。
